---
title: 贝尔曼公式 Bellman Equation
created: 2025-07-05 08:50:17
updated: 2025-11-29 10:09:24
---
## 引入：计算Return(Discounted)的方法

### 法1：根据定义

![PixPin_2025-07-06_08-55-44.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751763361124_PixPin_2025-07-06_08-55-44.png)

### 法2：==自举 Bootstraping==

![PixPin_2025-07-06_08-58-03.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751763495776_PixPin_2025-07-06_08-58-03.png)

从一个状态出发得到的return依赖于从其他状态出发得到的return
写为矩阵向量形式：

![PixPin_2025-07-06_09-01-16.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751763699218_PixPin_2025-07-06_09-01-16.png)

即为特定条件的确定性问题上的**贝尔曼公式**


## 状态值 State value

考虑如下单步状态转换过程：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}$$
**此处$S$、$R$、$A$都为随机变量**

- $S_t \rightarrow A_t$ 由策略 $\pi(A_t=a|S_t=s)$ 决定
- $S_t,A_t \rightarrow R_{t+1}$ 由 $p(R_{t+1}=r|S_t=s,A_t=a)$ 决定
- $S_t,A_t \rightarrow S_{t+1}$ 由 $p(S_{t+1}=s'|S_t=s,A_t=a)$ 决定

推广到多步的trajectory：
$$S_t\xrightarrow{A_t}R_{t+1},S_{t+1}\xrightarrow{A_{t+1}}R_{t+2},S_{t+2}\xrightarrow{A_{t+2}}R_{t+3},\ldots$$
==对应的discounted return $G_t$==
$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots$$
### 定义

状态值State value，为在策略$\pi$下，==**从状态$s$出发得到的$G_t$的期望**==
$$v_\pi(s)=\mathbb{E}[G_t|S_t=s]$$
可以表征状态的价值 
**确定性策略的状态值和**
![PixPin_2025-07-06_09-56-26.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751767006334_PixPin_2025-07-06_09-56-26.png)


## 贝尔曼公式 Bellman Equation

描述不同状态的State value之间的关系
### 推导

考虑随机变量trajectory：
$$S_t \xrightarrow{A_t} R_{t+1},S_{t+1} \xrightarrow{A_{t+1}}R_{t+1},S_{t+2} \xrightarrow{A_{t+2}}R_{t+3}, \dots$$
则：
$$
\begin{aligned}
G_t &= R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + \dots, \\
 &= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dots), \\
 &= R_{t+1} + \gamma G_{t+1}
\end{aligned}
$$
则根据State value定义，有：
$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&= \mathbb{E}[R_{t+1} | S_t = s ] + \gamma \mathbb{E}[G_{t+1} | S_t = s]
\end{aligned}
$$
^bellmanequation

其中$\mathbb{E}[R_{t+1} | S_t = s]$计算如下：
$$
\begin{aligned}
\mathbb{E}[R_{t+1} | S_t = s] &= \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t = s, A_t = a] \\ 
&= \sum_a  \pi(a|s) \cdot \left[\sum_r p(r|s,a) \cdot r \right] \
\end{aligned}
$$

> [!info] 理解
> - $\pi(a|s) = p(A_t = a | S_t = s)$
> - **$R_{t+1}$是$S_t$与$A_t$的函数**
> - $\mathbb{E}[R_{t+1} | S_t = s]$在状态$s$进行一步action获得reward的期望

$\mathbb{E}[G_{t+1} | S_t = s]$计算如下：

$$
\begin{aligned}
\mathbb{E}[G_{t+1} | S_t = s] &= \sum_{s'} \mathbb{E}[G_{t+1} | S_t = s, S_{t+1} = s'] \cdot p(s' | s) \\
&= \sum_{s'} \mathbb{E}[G_{t+1} | S_{t+1} = s'] \cdot p(s'|s) \\
&= \sum_{s'} v_{\pi}(s') \cdot p(s'|s) \\
&= \sum_{s'} v_{\pi}(s') \cdot \sum_a p(s' | s, a)\pi(a|s)
\end{aligned}
$$

> [!info] 理解
> - $\mathbb{E}[G_{t+1} | S_t = s]$是对于当前状态$s$，其可能的下一步状态$s'$得到的$G_{t+1}$的期望
> - $\mathbb{E}[G_{t+1} | S_t = s, S_{t+1} = s']$与$\mathbb{E}[G_{t+1} | S_{t+1} = s']$相等的依据[[计算机视觉/基本概念#马尔可夫决策过程 Markov Decision Process (MDP)]]中的*Markov property*，即==与历史无关的性质==。

故有贝尔曼公式：
$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E}[R_{t+1} | S_t = s] + \gamma \mathbb{E}[G_{t+1} | S_t = s] \\
&= \sum_a  \pi(a|s) \cdot \left[\sum_r p(r|s,a) \cdot r \right] + \gamma \cdot \sum_{s'} v_{\pi}(s') \cdot \sum_a p(s' | s, a)\pi(a|s) \\
&= \sum_a \pi(a|s) \left[ \sum_r p(r|s,a) + \gamma \sum_{s'}p(s' | s,a) v_{\pi}(s') \right], \forall{s} \in \mathcal{S}.
\end{aligned}
$$

>[!info] 理解
> - 状态值$v_{\pi}(s)$依赖于其他状态的状态值 -> 通过自举Bootstrapping计算
> - $\pi(a|s)$表示策略，$p(r|s,a)$和$p(s'|s,a)$是环境模型的内容

> [!help] 疑问
> 第二项中如何提出policy $\pi(a|s)$ ？

#### 示例1
![PixPin_2025-07-11_10-09-28.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1752199787801_PixPin_2025-07-11_10-09-28.png)

则有方程组
$$
\begin{aligned}
& v_{\pi}(s_1) = 0 + \gamma v_{\pi}(s_3), \\
& v_{\pi}(s_2) = 1 + \gamma v_{\pi}(s_4), \\
& v_{\pi}(s_3) = 1 + \gamma v_{\pi}(s_4), \\
& v_{\pi}(s_4) = 1 + \gamma v_{\pi}(s_4). 
\end{aligned}
$$
#### 示例2

![PixPin_2025-07-06_16-35-58.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751790978257_PixPin_2025-07-06_16-35-58.png)

$$
\begin{aligned}
 v_{\pi}(s_1) &= 0.5 \cdot [-1 + \gamma v_{\pi}(s_2)] + 0.5 \cdot [0 + \gamma v_{\pi}(s_3)] \\
&= -0.5 + 0.5\gamma[v_{\pi}(s_2) + v_{\pi}(s_3)], \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4), \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4)
\end{aligned}
$$
### 矩阵向量形式

贝尔曼公式重写
$$
\begin{aligned}
v_{\pi}(s) &= \sum_a \pi(a|s) \left[ \sum_r p(r|s,a) + \gamma \sum_{s'}p(s' | s,a) v_{\pi}(s') \right], \forall{s} \in \mathcal{S}. \\
&= r_{\pi}(s) + \gamma \sum_{s'}p_{\pi}(s' | s)v_{\pi}(s') 
\end{aligned}
$$
其中
$$
\begin{aligned}
r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s, a) r, \\
p_\pi(s' | s) \triangleq \sum_a \pi(a | s) p(s' | s, a). \\
\end{aligned}
$$
令状态$s_i (i=1,2,...n).$则对$s_i$的贝尔曼公式为
$$
v_{\pi}(s_i) = r_{\pi}(s) + \gamma \sum_{s_j}p_{\pi}(s_j | s_i)v_{\pi}(s_j) 
$$
则其**矩阵向量形式为**
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi
$$

> [!info] 理解
> 
> - $v_\pi = [v_\pi(s_1),...,v_\pi(s_n)]^T$
> - $r_\pi = [r_\pi(s_1),...,r_\pi(s_n)]^T$
> - $P_\pi  \in \mathbb{R}^{n \times n}$ 其中$[P_\pi]_{ij} = p_\pi(s_j|s_i)$，为**状态转移矩阵** 
^StateTransferMatrix

#### 示例1

![PixPin_2025-07-11_10-09-28.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1752199787801_PixPin_2025-07-11_10-09-28.png)

#### 示例2

![PixPin_2025-07-11_10-11-26.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1752199904325_PixPin_2025-07-11_10-11-26.png)


### 求解状态值

给定一个策略，求出对应状态值的过程称为**Policy Evaluation 策略评估**
#### 直接求解
$$v_\pi = \left(I - \gamma P_\pi\right)^{-1} r_\pi$$
- 需要求逆，计算量较大
#### 迭代法

通过迭代
$$
\begin{align*}
v_{k+1}=r_\pi+\gamma P_\pi v_k
\end{align*}
$$
得到序列$\{v_0,v_1,...\}$，可以证明
$$
\begin{align*}
v_k \to v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi}, \quad k \to \infty
\end{align*}
$$

> [!info]+ 证明过程
> 
> ![PixPin_2025-07-12_09-12-18.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1752282749047_PixPin_2025-07-12_09-12-18.png)


## 动作值 Action value

### 定义

agent从一个state出发选择某个action之后能得到的平均return
记作$q_\pi(s,a)$，依赖于策略$\pi$
$$
\begin{align*}
q_\pi(s, a) = \mathbb{E}[G_t|S_t = s, A_t = a]
\end{align*}
$$
### 与状态值的联系

$$

\begin{align*}
\mathbb{E}[G_t|S_t = s] = \sum_a \mathbb{E} \left[G_t|S_t = s, A_t = a \right] \pi(a|s)
\end{align*}
$$
即 
$$
v_{\pi}(s) = \sum \pi(a|s) q_{\pi}(s, a)
$$
$$
\begin{align*}
q_\pi(s, a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s' |s, a) v_\pi(s')
\end{align*}
$$
> [!info] 其中 
> -  $\sum_r p(r|s, a) r$是从状态$s$采取动作$a$的reward期望
> 
### 示例
![image.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1752287059629_20250712102410816.png)
- 碰壁的reward为-1
则对于$s_1$
$$
\begin{align*}
& q_{\pi}(s_1, a_1) = -1 + \gamma v_{\pi}(s_1), \\
& q_{\pi}(s_1, a_2) = -1 + \gamma v_{\pi}(s_2), \\
& q_{\pi}(s_1, a_3) = 0 + \gamma v_{\pi}(s_3), \\
& q_{\pi}(s_1, a_4) = -1 + \gamma v_{\pi}(s_1), \\
& q_{\pi}(s_1, a_2) = 0 + \gamma v_{\pi}(s_1).
\end{align*}
$$