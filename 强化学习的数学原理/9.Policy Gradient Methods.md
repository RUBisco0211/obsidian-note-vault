---
title: 9.Policy Gradient Methods
created: 2025-07-05 09:32:41
updated: 2025-11-29 11:26:13
aliases:
---
## 从基于值的方法到基于策略的方法

- value based：通过优化与值（状态值或动作值）相关的目标函数来得到最优值，从而得到目标策略
- policy based：直接优化与策略相关的目标函数来得到最优策略


## 基本概念

### 策略的连续形式

目前遇到的策略都是以表格的离散形式表示

![image.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1763262312617_20251116110502371.png)

要使用基于策略的方法，先将策略使用函数表示

$$
\pi(a | s, \theta), \quad \text{或} \quad \pi_{\theta}(a | s)
$$

其中$\theta \in \mathbb{R}^m$为参数向量

- 常见形式为神经网络：输入状态$s$，输出$s$下选择每个动作$a$的概率向量
策略函数的优势
- 可以表示连续状态和动作空间下的可能性
- 泛化能力

### 与离散表示的差异

1. 最优策略的定义
	- 离散：最优策略下的值最优
	- 连续：用标量指标来评估策略，最佳即为最优
2. 如何获取$\pi(a|s)$概率值？
	- 离散：查表
	- 连续：输入网络进行计算
3. 如何更新策略？
	- 离散：直接更新策略表格项
	- 连续：优化目标函数，更新参数$\theta$从而间接更新策略
### 基本思路

1. 使用$\theta$相关的指标或目标函数来定义最优策略：$J(\theta)$，需要最大化
2. 进行优化（基于梯度）：$\theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta} J(\theta_{t})$
   
> [!question] 问题
> 1. 如何选择指标或目标函数
> 2. 如何计算梯度？


## 定义最优策略的指标

主要有两种指标

### 1.平均状态值 average state value

$$
\bar{v}_{\pi} = \sum_{s \in \mathcal{S}} d(s) v_{\pi}(s) = \mathbb{E}[v_{\pi}(S)]
$$

其中$S \sim d$，也可写作矩阵向量形式

$$
\bar{v}_{\pi} = d^T v_{\pi}
$$
**另一种定义方法：**
$$
\bar{v}_{\pi} = \mathbb{E} \Big[   \sum_{t=0}^\infty \gamma^t R_{t+1} \Big ]
$$

证明：****

$$
\begin{align*}
\bar{v}_{\pi} &= \mathbb{E} \Big[   \sum_{t=0}^\infty \gamma^t R_{t+1} \Big ] \\
&= \sum_{s \in \mathcal{S}} d(s) \mathbb{E}\Big[ \sum_{t=0}^\infty \gamma^t R_{t+1} | S_{0} = s \Big] \\
&= \sum_{s \in \mathcal{S}} d(s) v_{\pi}(s)
\end{align*}
$$

#### 如何选择状态分布$d(s)$

**第一种情况：**$d$与$\pi$相互独立，记$d_{0} = d$，$\bar{v}_{\pi}^0 = \bar{v}_{\pi}$
（对策略求梯度时，由于$d$不依赖$\pi$，$\nabla  \bar{v}_{\pi} = d^T \nabla v_{\pi}$）
$d_{0}$的选择：
- 均匀分布，$d_{0} = 1 / |\mathcal{S}|$
- 仅关心某个状态如$s_{0}$，则$d_{0}(s_{0}) = 1, \quad d_{0}(s \neq s_{0}) = 0$

**第二种情况：** $d$依赖于$\pi$
- [[8.Value Function Approximation (VFA)#稳态分布 stationary distribution| 稳态分布 stationary distribution]]：$d = d_{\pi}$
	- 性质：$d_{\pi}^T P_{\pi} = d^T_{\pi}$，其中$P_{\pi}$为状态转移矩阵

### 2.平均奖励 average one-step reward

$$
\bar{r}_{\pi} = \sum_{s \in \mathcal{S}} d_{\pi}(s) r_{\pi}(s) = \mathbb{E}[r_{\pi}(S)]
$$

其中$S \sim d_{\pi}$，$d_{\pi}$为[[8.Value Function Approximation (VFA)#稳态分布 stationary distribution| 稳态分布]]，依赖于策略$\pi$。且其中

$$
r_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s)r(s,a)
$$
$$
r_{s,a} = \mathbb{E}[R | s,a] = \sum_{r} rp(r | s,a)
$$

即$r_{\pi}(s)$为在状态$s$下按照策略$\pi$选择动作$a$得到的平均单步奖励

**另一种定义方法：**
假设智能体按照给定的策略$\pi$生成一条trajectory $(R_{t+1}, R_{t + 2}, \dots)$
定义
$$
\begin{align*}

\bar{r}_{\pi} &= \lim_{ n \to \infty } \frac{1}{n} \mathbb{E}\big[ R_{t+1} + R_{t+2} + \dots + R_{t+n} \mid S_{t} = s_{0}\big] \\ 
&= \lim_{ n \to \infty } \frac{1}{n} \mathbb{E} \Big[  \sum_{k=1}^n R_{t+k} \mid S_{t} = s_{0} \Big] \\
&= \lim_{ n \to \infty } \frac{1}{n} \mathbb{E} \Big[  \sum_{k=1}^n R_{t+k} \Big] \\ 
&= \sum_{s} d_{\pi}(s)r_{\pi}(s)

\end{align*}
$$

当步数接近无穷时，起始状态$s_{0}$不重要

> [!important] 说明
> - $\text{metrics} \to \pi \to \theta$ 故可以通过优化metrics函数来得到最优的参数$\theta$
> - 难点：metrics可以在discounted case和undiscounted case种定义，即$\gamma \in [0,1)$或者$\gamma = 1$的两种情况
> - 直观上看，$\bar{r}_{\pi}$似乎比$\bar{v}_{\pi}$更短视。**但实际上二者等价，解出的最优策略是一致的。** 在discounted case的情况下，可以证明
> $$
> \bar{r}_{\pi} = (1-\gamma)\bar{v}_{\pi}
> $$


## 求解策略梯度

求解策略梯度最为复杂，需要考虑不同的目标函数以及discounted 和 undiscounted case 两种情况

一言以蔽之

$$
\nabla_{\theta} J(\theta) = \sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \underbrace{\nabla_{\theta} \pi(a | s, \theta)}_{\pi对\theta的梯度}  
\underbrace{q_{\pi}(s,a)}_{动作值} 
$$

$J(\theta)$可以为$\bar{v}_{\pi}, \bar{r}_{\pi}, \bar{v}_{\pi}^0$，分别与右式呈$=, \approx, \propto$关系

$\eta(x)$为状态的分布，不同目标函数中状态遵循不同的分布

例：

- $J(\theta) = \bar{r}_{\pi}$
$$
\nabla_{\theta} \bar{r}_{\pi} \simeq \sum_{s} d_{\pi}(s) \sum_{a} \nabla_{\theta} \pi(a | s,\theta) q_{\pi}(s,a)
$$

其中 discounted case 为约等于；undiscounted case 为严格等于

- $J(\theta) = \bar{v}_{\pi}$
$$
\nabla_{\theta} \bar{v}_{\pi} = \frac{1}{1-\gamma} \nabla_{\theta} \bar{r}_{\pi}
$$

discounted case下，如上公式

- $J(\theta) = \bar{v}_{\pi}^0$
$$
\nabla_{\theta} \bar{v}_{\pi}^0 = \sum_{s \in \mathcal{S}} \rho_\pi(s) \sum_{a \in \mathcal{A}} \nabla_{\theta} \pi(a | s,\theta) q_{\pi}(s,a)
$$

### 期望形式
$$
\begin{align*}

\nabla_{\theta} J(\theta) &= \sum_{s \in \mathcal{S}} \eta(s) \sum_{a \in \mathcal{A}} \nabla_{\theta} \pi(a | s, \theta) q_{\pi}(s,a) \\ \\
&= \mathbb{E}[\nabla_{\theta} \ln \pi(A | S, \theta) q_{\pi}(S,A)]

\end{align*}
$$

其中$S \sim \eta, A \sim \pi(A | S, \theta)$

在期望形式下，可以非常轻松地用**采样值**来近似估计梯度（随机梯度下降/上升）

$$
\nabla_{\theta} J \approx \nabla_{\theta} \ln \pi(a | s, \theta) q_{\pi}(s,a)
$$
> [!important] 期望形式推导
> 由
> $$
> \nabla_{\theta} \ln \pi(a | s, \theta) = \frac{\nabla_{\theta}\pi(a | s, \theta)}{\pi(a | s, \theta)}
> $$
> 故
> $$
> \begin{align*}{\nabla }_{\theta }J &= \mathop{\sum }\limits_{s}d\left( s\right) \mathop{\sum }\limits_{a}{\nabla }_{\theta }\pi \left( {a \mid  s,\theta }\right) {q}_{\pi }\left( {s,a}\right) \\ &= \mathop{\sum }\limits_{s}d\left( s\right) \mathop{\sum }\limits_{a}\pi \left( {a|s,\theta }\right) {\nabla }_{\theta }\ln \pi \left( {a|s,\theta }\right) {q}_{\pi }\left( {s,a}\right) \\&= \mathbb{E}_{S \sim d} \Big [ \sum_{a} \pi(a | S, \theta) \nabla_{\theta} \ln \pi(a | S, \theta) q_{\pi}(S,a) \Big] \\&= \mathbb{E}_{S \sim d, A \sim \pi} \Big[ \nabla_{\theta} \ln \pi(A | S, \theta) q_{\pi}(S,A) \Big]\end{align*} 
> $$
 >
 > 注意：考虑到$\ln \pi(a | s, \theta)$的存在性，需要保证$\pi(a | s, \theta) > 0$
 > 可以使用softmax函数来确保
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 > $$
 > \text{softmax}(x) = [z_{1}, \dots ,z_{n}]^T, \quad z_{i} = \frac{e^{ x_{i} }}{\sum_{j=1}^n e^{ x_{j} }}
 > $$
 > 则策略函数改写为
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 >
 > $$
 > \pi \left( {a \mid  s,\theta }\right)  = \frac{{e}^{h\left( {s,a,\theta }\right) }}{\mathop{\sum }\limits_{{{a}^{\prime } \in  \mathcal{A}}}{e}^{h\left( {s,{a}^{\prime },\theta }\right) }}
 > $$
 > 其中$h(s,a,\theta)$可以为一特征函数（神经网络的最后一层用softmax归一化）
 

## 梯度上升方法 (REINFORCE)

### 基本思路
$$
\begin{align*}
\theta_{t+1} &= \theta_{t} + \alpha \nabla_{\theta} J(\theta) \\ \\
&= \theta_{t} + \alpha \mathbb{E}\Big[ \nabla_{\theta} \ln \pi(A | S, \theta_{t}) q_{\pi}(S,A) \Big]
\end{align*}
$$

期望涉及到状态与动作的分布，无法直接求得

故用随机采样$(s_{t}, a_{t})$对应梯度代替真实梯度，有

$$
\theta_{t+1} = \theta_{t} + \alpha \nabla_{\theta} \ln \pi(a_{t} | s_{t}, \theta_{t}) q_{\pi}(s_{t}, a_{t})
$$

且其中$q_{\pi}(s_{t},a_{t})$也无法求得，故用$q_{t}(s_{t}, a_{t})$进行近似

- [[5.Monte-Carlo Methods（model-free）|Monte-Carlo]] 方法：从$(s,a)$出发得到一个trajectory的数据来估计动作值，这种方法即为**REINFORCE**
- [[7.Temporal-Difference Learning（TD learning）|TD]]方法：此种方法为[[10.Actor-Critic Methods|Actor-Critic]]
### 如何采样

- 对于$S$进行采样：由于$S \sim d$且$d$分布需要策略$\pi$下的长期数据，故一般不考虑遵循$d$分布
- 对于$A$进行采样：$a_{t}$的采样应该遵循$\pi(s_{t}, \theta)$，故这是一种**on-policy方法**
### 如何实现

重写迭代公式

$$
\begin{align*}
\theta_{t+1} &= \theta_{t} + \alpha \nabla_{\theta} J(\theta) \\ \\
&= \theta_{t} + \alpha \nabla_{\theta} \ln \pi(a_{t} | s_{t}, \theta_{t}) q_{t}(s_{t},a_{t})  \\ \\
&= \theta_{t} + \alpha \underbrace{\Bigg( \frac{q_{t}(s_{t},a_{t})}{\pi(a_{t} | s_{t}, \theta_{t})} \Bigg)}_{\beta_{t}}  \nabla_{\theta} \pi(s_{t} | s_{t, \theta_{t}}) \\ \\
&= \theta_{t} + \underbrace{\alpha \beta_{t}}_{\alpha'}  \nabla_{\theta} \pi(a_{t} | s_{t}, \theta_{t}) 
\end{align*}
$$
**当$\alpha \beta_{\theta}$很小时，把$\alpha \beta_{\theta}$看作一个新的步长，则上式可以看作在固定$a_{t},s_{t}$时，对$\pi(a_{t} | s_{t}, \theta)$进行优化**
- $\beta_{t} > 0$，则为梯度上升
- $\beta_{t} < 0$，则为梯度下降

> [!info] 通过$\beta_{t}$平衡exploration和exploitation
> - $\beta_{t}$与$q_{t}(s_{t},a_{t})$成正比：当$q_{t}(s_{t},a_{t})$较大时，$\beta_{t}$也较大，则一步迭代后$\pi(a_{t} | s_{t}, \theta)$会有较大增幅。即动作值越大，就给予该动作越大的选择概率。**exploitation**
> - $\beta_{t}$与$\pi(a_{t} | s_{t}, \theta_{t})$成反比：当$\pi(a_{t} | s_{t}, \theta_{t})$较小时，$\beta_{t}$较大，则一步迭代后$\pi(a_{t} | s_{t}, \theta)$增大。即若选择该动作的概率较小，就在下个时刻给予其更大的选择概率。**exploration**  
^NewStepSize
### 算法流程

![image.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1763287005042_20251116175638481.png)
