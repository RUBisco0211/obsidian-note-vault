
## 贝尔曼最优公式（BOE）

$$
v = \max_{\pi}(r_{\pi} + \gamma P_{\pi}v)
$$
---
## 值迭代 value iteration (VI)

基本思想：$v_{k+1} = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k}), k=1,2,3\dots$
### 算法思路（矩阵向量形式）
每次迭代包括两个步骤
1. 策略更新（policy update）：$$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k})$$，用状态值$v_{k}$计算策略$\pi_{k+1}$
2. 值更新（value update）：$$v_{k+1} = r_{\pi} + \gamma P_{\pi_{k+1}} v_{k}$$，用$\pi_{k+1}$计算新的状态值$v_{k+1}$

### 算法实现（分量形式）

- 初始化：$p(r | s,a), p(s' | s,a)$已知，给出初始状态值$v_{0}$
- 目标：求出使状态值最大的最优策略$\pi^*$
- 算法：
	
---
## 策略迭代 policy iteration (PI)

---
## truncated policy iteration


