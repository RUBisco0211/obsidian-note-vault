
## 贝尔曼最优公式（BOE）

$$
v = \max_{\pi}(r_{\pi} + \gamma P_{\pi}v)
$$
---
## 值迭代 value iteration (VI)

基本思想：$v_{k+1} = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k}), k=1,2,3\dots$
### 算法思路（矩阵向量形式）
每次迭代包括两个步骤
1. 策略更新（policy update）：$$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k})$$，用状态值$v_{k}$计算策略$\pi_{k+1}$
2. 值更新（value update）：$$v_{k+1} = r_{\pi} + \gamma P_{\pi_{k+1}} v_{k}$$，用$\pi_{k+1}$计算新的状态值$v_{k+1}$

### 算法实现（分量形式）

- 初始化：$p(r | s,a), p(s' | s,a)$已知，给出初始状态值$v_{0}$
- 目标：求出使状态值$V(s)$最大的最优策略$\pi^*$
- 迭代循环（当$\|v_{k} -v_{k-1} \| > threshold$时）：
	- 对于每个$s \in \mathcal{S}$：
		- 对于$s$上的每个动作$a \in \mathcal{A}(s)$：
			- 计算动作值$$q_{k}(s,a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s' |s, a) v_{k}(s')  $$
		- 求出动作值最大的$a^*(s) = \arg \max_{a} q_{k}(s,a)$
	- 策略更新：$$\pi_{k+1} = $$
---
## 策略迭代 policy iteration (PI)

---
## truncated policy iteration


