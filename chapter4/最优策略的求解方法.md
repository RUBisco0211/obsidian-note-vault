
## 贝尔曼最优公式（BOE）

$$
v = \max_{\pi}(r_{\pi} + \gamma P_{\pi}v)
$$
---
## 值迭代 value iteration (VI)

基本思想：$v_{k+1} = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k}), k=1,2,3\dots$

每次迭代包括两个步骤
1. 策略更新（policy update）：$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k})$，求出策略$\pi_{k+1}$
2. 值更新（value update）：$v_{k+1} = r_{\pi} + \gamma P_{\pi_{k+1}} v_{k}$，用$\pi_{k+1}$计算新的状态值

---
## 策略迭代 policy iteration (PI)

---
## truncated policy iteration


