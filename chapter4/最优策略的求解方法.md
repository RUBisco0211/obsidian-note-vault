## 贝尔曼最优公式（BOE）

$$
v = \max_{\pi}(r_{\pi} + \gamma P_{\pi}v)
$$
---
## 值迭代 value iteration (VI)

基本思想：$v_{k+1} = \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k}), k=1,2,3\dots$
### 算法思路（矩阵向量形式）

每次迭代包括两个步骤
- 策略更新（policy update）：$\pi_{k+1} = \arg \max_{\pi}(r_{\pi} + \gamma P_{\pi} v_{k})$，用状态值$v_{k}$计算策略$\pi_{k+1}$
- 值更新（value update）：$v_{k+1} = r_{\pi} + \gamma P_{\pi_{k+1}} v_{k}$，用$\pi_{k+1}$计算新的状态值$v_{k+1}$

### 算法实现（分量形式）

- 初始化：$p(r | s,a), p(s' | s,a)$已知，给出初始状态值$v_{0}$
- 目标：求出使状态值$V(s)$最大的最优策略$\pi^*$
- 迭代循环（第$k$次）（当 $\|v_{k} -v_{k-1} \| > threshold$ 时）：
	- 对于每个$s \in \mathcal{S}$：
		- 对于$s$上的每个动作$a \in \mathcal{A}(s)$：
			- 计算动作值$q_{k}(s,a) = \sum_r p(r|s, a) r + \gamma \sum_{s'} p(s' |s, a) v_{k}(s')$
		- 求出动作值最大的$a^*(s) = \arg \max_{a} q_{k}(s,a)$
	- 策略更新：$\pi_{k+1} (a \mid s)= \begin{cases} 1 \quad a = a^*_{k} \\ 0 \quad a \not = a^*_{k}\end{cases}$
	- 值更新：$v_{k+1}(s) = \max_{a \in \mathcal{A}(s)} q_k(a,s)$

---
## 策略迭代 policy iteration (PI)

### 算法思路（矩阵向量形式）

每次迭代包括两个步骤
- 策略评估（policy evaluation）：计算$v_{\pi_{k}} = r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}}$（贝尔曼公式），用策略计算当前状态值$v_{\pi_{k}}$
- 策略提升（policy improvement）：计算$\pi_{k+1} = \arg \max_{a}(r_{\pi} + \gamma P_{\pi} v_{\pi_{k}})$，用$v_{\pi_{k}}$计算新的更优的策略（可证明）
### 算法实现（分量形式）

- 初始化：$p(r | s,a), p(s' | s,a)$已知，给出初始策略$\pi_{0}$
- 目标：求出使状态值$V(s)$最大的最优策略$\pi^*$
- 迭代循环（第$k$次）（当 $\|v_{k} -v_{k-1} \| > threshold$ 时）：
	- 策略评估：用贝尔曼公式计算当前策略下的状态值
---
## truncated policy iteration


