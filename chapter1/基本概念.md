## 例子：网格世界

![PixPin_2025-07-05_11-07-54.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751684883672_PixPin_2025-07-05_11-07-54.png)

---
## 智能体 Agent
## 环境 Environment

---
## 状态 State

记作$s_i$
**状态空间 State Space** 记作 $\mathcal{S}=\{s_i\}_{i=1}^{9}$

![PixPin_2025-07-05_11-06-42.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751684809860_PixPin_2025-07-05_11-06-42.png)

---
## 动作 Action

![PixPin_2025-07-05_11-14-06.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751685264980_PixPin_2025-07-05_11-14-06.png)

一个状态转换到下一个状态的可能方式$a_i$
定义**状态转换 State Transition** $s_{1}\xrightarrow{a_{2}}s_{2}$
状态$s_i$上的**动作空间 Action Space** 记作$\mathcal{A}(s_i) = \{a_i\}_{i=0}^5$ ，仅依赖于状态

### 用==条件概率==描述状态转换

如对于$s_{1}\xrightarrow{a_{2}}s_{2}$，记作
$$\begin{aligned}
 & p(s_2|s_1,a_2)=1 \\
 & p(s_i|s_1,a_2)=0\quad\forall i\neq2
\end{aligned}
$$
- **使用条件概率可以描述涉及==随机性==的状态转换**
---
## 策略 Policy

告知智能体在某一状态时选择某一动作的可能性
![PixPin_2025-07-05_19-21-03.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751714472449_PixPin_2025-07-05_19-21-03.png)
如对于状态$s_1$，其策略可记作
$$\begin{aligned} 
& \pi(a_1|s_1) = 0 \\
& \pi(a_2|s_1) = 1 \\
& \pi(a_3|s_1) = 0 \\
& \pi(a_4|s_1) = 0 \\
& \pi(a_5|s_1) = 0 \\
\end{aligned}
$$
为**确定性策略 Deterministic Policy**

也有**不确定性策略 Stochastic Policy**
![PixPin_2025-07-05_19-32-09.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751715136039_PixPin_2025-07-05_19-32-09.png)

---
## 奖励 Reward

对采取的动作给出的一个指标值，为正表示动作值得鼓励，为负表示动作应该受到惩罚（一般而言），记作$r$

![PixPin_2025-07-05_19-49-43.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751716197352_PixPin_2025-07-05_19-49-43.png)

![PixPin_2025-07-05_19-53-29.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751716416021_PixPin_2025-07-05_19-53-29.png)

同样可以使用条件概率表示在某一状态下采取某一动作的奖励，记作
$$\begin{aligned}
& p(r=-1|s_1,a_1)=1 \\
& p(r\neq-1|s_1,a_1)=0 \\
\end{aligned}$$
**奖励值依赖于==当前状态==以及==采取的动作==，而不是下一个状态**

---
## 轨迹 Trajectory

从一个状态到另一个状态的状态、动作和奖励链
## 回馈 Return

一个==trajectory==上所有Reward的和，评估策略优劣的重要指标

![PixPin_2025-07-05_20-12-17.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751717547642_PixPin_2025-07-05_20-12-17.png)

![PixPin_2025-07-05_20-15-16.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751717723642_PixPin_2025-07-05_20-15-16.png)

---
## Discounted Return

目的：解决无限长trajectory发散的问题

![PixPin_2025-07-05_20-17-23.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751717856828_PixPin_2025-07-05_20-17-23.png)

引入**折扣率 discount rate $\gamma \in [0,1)$**，则

$$
\begin{aligned}
discounted\ return & =0+\gamma0+\gamma^{2}0+\gamma^{3}1+\gamma^{4}1+\gamma^{5}1+\ldots \\
 & =\gamma^{3}(1+\gamma+\gamma^{2}+\ldots)=\gamma^{3}\frac{1}{1-\gamma}.
\end{aligned}
$$
![PixPin_2025-07-05_20-20-31.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1751718043809_PixPin_2025-07-05_20-20-31.png)

一般来说
- $\gamma$ 越接近0，agent会更注重最近的一些奖励
- $\gamma$ 越接近1，agent会更加远视 

---
## Episode

从初始状态到终止状态的一条trajectory称为一个episode或trial，通常为有限步
- episodic task: 智能体和环境的交互可以在有限步内完成
- continuing task: 智能体和环境的交互会永久持续下去

将episodic任务转化为continuing任务的方式：
- 将目标状态的动作设为留在原地，且对应的奖励为0
- 将目标状态看作普通状态，且智能体可以离开目标状态，进入目标状态时获得+1的奖励值（更为一般化）

---
## 马尔可夫决策过程 Markov Decision Process (MDP)

MDP的要素：
- 集合
	- States 状态空间：$\mathcal{S}$
	- Actions 某状态上的动作空间：$\mathcal{A}(s), s \in \mathcal{S}$
	- Rewards 某状态上某动作的奖励值：$\mathcal{R}(s,a)$
- 概率分配
	- State transition probability 状态转换的条件概率：$p(s'|s,a)$
	- Reward probability 动作奖励的条件概率：$p(r|s,a)$
- Policy 策略：$\pi(a|s)$
- ==***Markov property***：与历史无关的性质==
$$\begin{aligned} 
& p(s_{t+1}|a_{t+1},s_{t},\ldots,a_{1},s_{0})=p(s_{t+1}|a_{t+1},s_{t}) \\
& p(r_{t+1}|a_{t+1},s_{t},\ldots,a_{1},s_{0})=p(r_{t+1}|a_{t+1},s_{t}) \\
\end{aligned}$$
---