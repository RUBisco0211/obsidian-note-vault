## 从基于值的方法到基于策略的方法

- value based：通过优化与值（状态值或动作值）相关的目标函数来得到最优值，从而得到目标策略
- policy based：直接优化与策略相关的目标函数来得到最优策略

---
## 基本思路

### 策略的连续形式

目前遇到的策略都是以表格的离散形式表示

![image.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1763262312617_20251116110502371.png)

要使用基于策略的方法，先将策略使用函数表示
$$\pi(a | s, \theta), \quad \text{或} \quad \pi_{\theta}(a | s)$$
其中$\theta \in \mathbb{R}^m$为参数向量
- 常见形式为神经网络：输入状态$s$，输出$s$下选择每个动作$a$的概率向量
策略函数的优势
- 可以表示连续状态和动作空间下的可能性
- 泛化能力

### 与离散表示的差异

- 最优策略的定义
	- 离散：最优策略下的值最优
	- 连续：用标量指标来评估策略，最佳即为最优
- 如何获取$\pi(a|s)$概率值？
	- 离散：查表
	- 连续：输入网络进行计算
- 如何更新策略？
	- 离散：直接更新策略表格项
	- 连续：优化目标函数，更新参数$\theta$从而间接更新策略
---
## 定义最优策略的指标


---
## 求解策略梯度

---
## 梯度上升方法 (REINFORCE)