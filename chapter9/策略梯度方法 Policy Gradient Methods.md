## 从基于值的方法到基于策略的方法

- value based：通过优化与值（状态值或动作值）相关的目标函数来得到最优值，从而得到目标策略
- policy based：直接优化与策略相关的目标函数来得到最优策略

---
## 基本思路

目前遇到的策略都是以表格的离散形式表示

![image.png](https://cloudflare-imgbed-1v8.pages.dev/file/img/note/rl/1/1763262312617_20251116110502371.png)

要使用基于策略的方法，先将策略使用函数表示
$$\pi(a | s, \theta)$$
其中$\theta \in \mathbb{R}^m$为参数向量
- 常见形式为神经网络：输入状态$s$，输出$s$下选择每个动作$a$的概率向量
策略函数的优势
- 可以表示连续状态和动作空间下的可能性
- 泛化能力

---
### 定义最优策略的指标

---
### 求解策略梯度

---
### 梯度上升方法 (REINFORCE)